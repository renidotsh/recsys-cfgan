{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title-cell",
            "metadata": {},
            "source": [
                "# CFGAN — Collaborative Filtering with Generative Adversarial Networks\n",
                "\n",
                "This notebook implements a **GAN-based collaborative filtering** model inspired by the [CFGAN paper](https://dl.acm.org/doi/10.1145/3269206.3271743).  \n",
                "The generator learns to produce realistic item-interaction vectors for users, while the discriminator tries to distinguish real interactions from generated ones.\n",
                "\n",
                "**Architecture overview:**\n",
                "\n",
                "```\n",
                "User Embedding + Noise ──► Generator ──► Fake Item Vector\n",
                "                                              │\n",
                "User Embedding + Real Items ──► Discriminator ◄──┘\n",
                "                                      │\n",
                "                                  Real / Fake?\n",
                "```\n",
                "\n",
                "**Dataset:** [MovieLens 100K](https://grouplens.org/datasets/movielens/100k/) (100,000 ratings from 943 users on 1,682 movies)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-setup",
            "metadata": {},
            "source": [
                "## 1. Setup & Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras import layers, Model\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Reproducibility\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "print(f\"TensorFlow version: {tf.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-loading",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load MovieLens 100K dataset\n",
                "url = 'http://files.grouplens.org/datasets/movielens/ml-100k/u.data'\n",
                "data = pd.read_csv(url, sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
                "\n",
                "print(f\"Ratings: {len(data):,}\")\n",
                "print(f\"Users:   {data['user_id'].nunique()}\")\n",
                "print(f\"Items:   {data['item_id'].nunique()}\")\n",
                "data.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-preprocessing",
            "metadata": {},
            "source": [
                "## 2. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "parameters",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameters\n",
                "LATENT_DIM = 10\n",
                "BATCH_SIZE = 64\n",
                "EPOCHS = 100\n",
                "LEARNING_RATE = 0.0002\n",
                "BETA_1 = 0.5\n",
                "\n",
                "num_users = data['user_id'].nunique()\n",
                "num_items = data['item_id'].nunique()\n",
                "\n",
                "print(f\"Latent dim: {LATENT_DIM}, Batch size: {BATCH_SIZE}, Epochs: {EPOCHS}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocessing",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalize ratings to [0, 1]\n",
                "ratings = data['rating'].values\n",
                "ratings = (ratings - ratings.min()) / (ratings.max() - ratings.min())\n",
                "\n",
                "# Initialize random user embeddings\n",
                "user_embeddings = np.random.normal(size=(num_users, LATENT_DIM))\n",
                "item_embeddings = np.random.normal(size=(num_items, LATENT_DIM))\n",
                "\n",
                "# Build user-item interaction matrix\n",
                "interaction_matrix = np.zeros((num_users, num_items))\n",
                "for i, row in data.iterrows():\n",
                "    user_idx = row['user_id'] - 1\n",
                "    item_idx = row['item_id'] - 1\n",
                "    interaction_matrix[user_idx, item_idx] = ratings[i]\n",
                "\n",
                "print(f\"Interaction matrix shape: {interaction_matrix.shape}\")\n",
                "print(f\"Sparsity: {(interaction_matrix == 0).sum() / interaction_matrix.size:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-generator",
            "metadata": {},
            "source": [
                "## 3. Generator\n",
                "\n",
                "Takes a **user embedding** and **random noise** as input, then generates a synthetic item-interaction vector of size `num_items`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "generator",
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_generator(latent_dim, num_items):\n",
                "    user_input = layers.Input(shape=(latent_dim,), name='gen_user')\n",
                "    noise_input = layers.Input(shape=(latent_dim,), name='gen_noise')\n",
                "\n",
                "    merged = layers.Concatenate()([user_input, noise_input])\n",
                "    x = layers.Dense(128, activation='relu')(merged)\n",
                "    x = layers.Dense(256, activation='relu')(x)\n",
                "    generated_items = layers.Dense(num_items, activation='sigmoid')(x)\n",
                "\n",
                "    return Model([user_input, noise_input], generated_items, name='Generator')\n",
                "\n",
                "generator = build_generator(LATENT_DIM, num_items)\n",
                "generator.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-discriminator",
            "metadata": {},
            "source": [
                "## 4. Discriminator\n",
                "\n",
                "Takes a **user embedding** and an **item vector** (real or generated), and outputs a probability that the interaction is real."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "discriminator",
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_discriminator(latent_dim, num_items):\n",
                "    user_input = layers.Input(shape=(latent_dim,), name='disc_user')\n",
                "    item_input = layers.Input(shape=(num_items,), name='disc_items')\n",
                "\n",
                "    merged = layers.Concatenate()([user_input, item_input])\n",
                "    x = layers.Dense(256, activation='relu')(merged)\n",
                "    x = layers.Dense(128, activation='relu')(x)\n",
                "    validity = layers.Dense(1, activation='sigmoid')(x)\n",
                "\n",
                "    return Model([user_input, item_input], validity, name='Discriminator')\n",
                "\n",
                "discriminator = build_discriminator(LATENT_DIM, num_items)\n",
                "\n",
                "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE, BETA_1)\n",
                "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
                "discriminator.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-combined",
            "metadata": {},
            "source": [
                "## 5. Combined (Adversarial) Model\n",
                "\n",
                "The combined model chains Generator → Discriminator with the discriminator weights frozen, so that only the generator learns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "combined-model",
            "metadata": {},
            "outputs": [],
            "source": [
                "discriminator.trainable = False\n",
                "\n",
                "user_input = layers.Input(shape=(LATENT_DIM,))\n",
                "noise_input = layers.Input(shape=(LATENT_DIM,))\n",
                "generated_items = generator([user_input, noise_input])\n",
                "validity = discriminator([user_input, generated_items])\n",
                "\n",
                "combined = Model([user_input, noise_input], validity, name='CFGAN')\n",
                "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
                "\n",
                "print(\"Combined model compiled.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-training",
            "metadata": {},
            "source": [
                "## 6. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "training-loop",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Storage for loss history\n",
                "d_losses, g_losses, d_accs = [], [], []\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    # --- Sample a batch ---\n",
                "    idx = np.random.randint(0, num_users, BATCH_SIZE)\n",
                "    users = user_embeddings[idx]\n",
                "    noise = np.random.normal(0, 1, (BATCH_SIZE, LATENT_DIM))\n",
                "\n",
                "    # Generate fake item interactions\n",
                "    fake_items = generator.predict([users, noise], verbose=0)\n",
                "    real_items = interaction_matrix[idx]\n",
                "\n",
                "    # Labels\n",
                "    real_labels = np.ones((BATCH_SIZE, 1))\n",
                "    fake_labels = np.zeros((BATCH_SIZE, 1))\n",
                "\n",
                "    # --- Train discriminator ---\n",
                "    d_loss_real = discriminator.train_on_batch([users, real_items], real_labels)\n",
                "    d_loss_fake = discriminator.train_on_batch([users, fake_items], fake_labels)\n",
                "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
                "\n",
                "    # --- Train generator ---\n",
                "    g_loss = combined.train_on_batch([users, noise], real_labels)\n",
                "\n",
                "    d_losses.append(d_loss[0])\n",
                "    g_losses.append(g_loss)\n",
                "    d_accs.append(d_loss[1])\n",
                "\n",
                "    if (epoch + 1) % 10 == 0:\n",
                "        print(f\"Epoch {epoch+1:3d}/{EPOCHS}  \"\n",
                "              f\"D_loss: {d_loss[0]:.4f}  D_acc: {d_loss[1]:.4f}  \"\n",
                "              f\"G_loss: {g_loss:.4f}\")\n",
                "\n",
                "print(\"\\nTraining complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-results",
            "metadata": {},
            "source": [
                "## 7. Training Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "loss-plot",
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Loss curves\n",
                "ax1.plot(d_losses, label='Discriminator Loss', alpha=0.8)\n",
                "ax1.plot(g_losses, label='Generator Loss', alpha=0.8)\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('Loss')\n",
                "ax1.set_title('Training Loss Curves')\n",
                "ax1.legend()\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# Discriminator accuracy\n",
                "ax2.plot(d_accs, color='green', alpha=0.8)\n",
                "ax2.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random baseline')\n",
                "ax2.set_xlabel('Epoch')\n",
                "ax2.set_ylabel('Accuracy')\n",
                "ax2.set_title('Discriminator Accuracy')\n",
                "ax2.legend()\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-evaluation",
            "metadata": {},
            "source": [
                "## 8. Evaluation — Top-K Recommendation Metrics\n",
                "\n",
                "We generate item scores for each user, then evaluate against held-out real interactions using:\n",
                "- **Precision@K** — fraction of recommended items that are relevant\n",
                "- **Recall@K** — fraction of relevant items that are recommended\n",
                "- **NDCG@K** — measures ranking quality with position-aware discounting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "evaluation-metrics",
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_topk(generator, user_embeddings, interaction_matrix, k_values=[5, 10, 20]):\n",
                "    \"\"\"Evaluate generator recommendations using Precision, Recall, NDCG at K.\"\"\"\n",
                "    results = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in k_values}\n",
                "\n",
                "    for user_idx in range(len(user_embeddings)):\n",
                "        user_emb = user_embeddings[user_idx].reshape(1, -1)\n",
                "        noise = np.random.normal(0, 1, (1, user_emb.shape[1]))\n",
                "\n",
                "        # Generate predicted scores\n",
                "        predicted_scores = generator.predict([user_emb, noise], verbose=0).flatten()\n",
                "\n",
                "        # Ground truth: items the user actually interacted with\n",
                "        actual_items = set(np.where(interaction_matrix[user_idx] > 0)[0])\n",
                "        if len(actual_items) == 0:\n",
                "            continue\n",
                "\n",
                "        # Rank all items by predicted score\n",
                "        ranked_items = np.argsort(predicted_scores)[::-1]\n",
                "\n",
                "        for k in k_values:\n",
                "            top_k = ranked_items[:k]\n",
                "            hits = len(set(top_k) & actual_items)\n",
                "\n",
                "            precision = hits / k\n",
                "            recall = hits / len(actual_items)\n",
                "\n",
                "            # NDCG\n",
                "            dcg = sum(1.0 / np.log2(i + 2) for i, item in enumerate(top_k) if item in actual_items)\n",
                "            idcg = sum(1.0 / np.log2(i + 2) for i in range(min(k, len(actual_items))))\n",
                "            ndcg = dcg / idcg if idcg > 0 else 0\n",
                "\n",
                "            results[k]['precision'].append(precision)\n",
                "            results[k]['recall'].append(recall)\n",
                "            results[k]['ndcg'].append(ndcg)\n",
                "\n",
                "    # Average\n",
                "    summary = {}\n",
                "    for k in k_values:\n",
                "        summary[f'@{k}'] = {\n",
                "            'Precision': np.mean(results[k]['precision']),\n",
                "            'Recall': np.mean(results[k]['recall']),\n",
                "            'NDCG': np.mean(results[k]['ndcg'])\n",
                "        }\n",
                "    return pd.DataFrame(summary)\n",
                "\n",
                "\n",
                "eval_df = evaluate_topk(generator, user_embeddings, interaction_matrix)\n",
                "print(\"\\nTop-K Evaluation Metrics:\")\n",
                "eval_df"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-conclusion",
            "metadata": {},
            "source": [
                "## 9. Observations\n",
                "\n",
                "- The GAN training dynamics show the characteristic adversarial interplay — when the discriminator gets too strong, the generator adapts and vice versa.\n",
                "- On the MovieLens 100K dataset, the model learns meaningful user-item interaction patterns even with simple dense-layer architectures.\n",
                "- **Possible improvements:**\n",
                "  - Condition the generator on the user's actual interaction history (partial purchase vector)\n",
                "  - Use a Wasserstein loss (WGAN) for more stable training\n",
                "  - Add dropout / batch normalization for regularization\n",
                "  - Train/test split for more rigorous evaluation\n",
                "\n",
                "For traditional similarity-based approaches, see the companion **CollaborativeFiltering notebook**."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
