{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title-cell",
            "metadata": {},
            "source": [
                "# Collaborative Filtering — User-User Similarity Analysis\n",
                "\n",
                "This notebook implements and compares three user-user similarity metrics for collaborative filtering:\n",
                "\n",
                "| Metric | Description | Range |\n",
                "|--------|-------------|-------|\n",
                "| **Mean Squared Difference (MSD)** | Measures average squared rating difference between users | [0, 1] after transformation |\n",
                "| **Pearson Correlation** | Captures linear correlation between co-rated items | [-1, 1] → normalized to [0, 1] |\n",
                "| **Cosine Similarity** | Measures angle between user rating vectors | [0, 1] |\n",
                "\n",
                "We visualize each similarity matrix as a heatmap and use the best-performing metric to generate rating predictions via **k-Nearest Neighbors (k-NN)**."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-imports",
            "metadata": {},
            "source": [
                "## 1. Setup & Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "from scipy.stats import pearsonr\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.colors import LinearSegmentedColormap\n",
                "\n",
                "# Reproducibility\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-loading-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the sparse ratings dataset\n",
                "column_names = ['userId', 'movieId', 'rating', 'timestamp']\n",
                "data = pd.read_csv('../data/rating_sparse.csv', names=column_names, skiprows=1)\n",
                "data.drop('timestamp', axis=1, inplace=True)\n",
                "\n",
                "# Build user-item rating matrix (NaN = unrated)\n",
                "rating_matrix = data.pivot(index='userId', columns='movieId', values='rating')\n",
                "\n",
                "print(f\"Users: {rating_matrix.shape[0]}, Items: {rating_matrix.shape[1]}\")\n",
                "print(f\"Sparsity: {rating_matrix.isna().sum().sum() / rating_matrix.size:.2%}\")\n",
                "rating_matrix.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-similarity-functions",
            "metadata": {},
            "source": [
                "## 2. Similarity Functions\n",
                "\n",
                "Each function operates on a pair of user rating vectors and only considers **co-rated items** (items both users have rated)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "msd-function",
            "metadata": {},
            "outputs": [],
            "source": [
                "def mean_square_difference(user1, user2):\n",
                "    \"\"\"Compute MSD between two users over their co-rated items.\n",
                "    \n",
                "    Returns np.inf when no items are co-rated (maps to 0 after\n",
                "    the 1/(1+MSD) transformation).\n",
                "    \"\"\"\n",
                "    common_items = user1.dropna().index.intersection(user2.dropna().index)\n",
                "    if len(common_items) == 0:\n",
                "        return np.inf\n",
                "    return np.mean((user1[common_items] - user2[common_items]) ** 2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "pearson-function",
            "metadata": {},
            "outputs": [],
            "source": [
                "def pearson_similarity(user1, user2):\n",
                "    \"\"\"Compute Pearson correlation between two users over co-rated items.\n",
                "    \n",
                "    Returns 0 when there are no co-rated items or when standard\n",
                "    deviation is zero (constant ratings).\n",
                "    \"\"\"\n",
                "    common_items = user1.dropna().index.intersection(user2.dropna().index)\n",
                "    if len(common_items) == 0:\n",
                "        return 0\n",
                "\n",
                "    r1 = user1[common_items].values\n",
                "    r2 = user2[common_items].values\n",
                "\n",
                "    mean1, mean2 = np.mean(r1), np.mean(r2)\n",
                "    numerator = np.sum((r1 - mean1) * (r2 - mean2))\n",
                "    denominator = np.sqrt(np.sum((r1 - mean1) ** 2)) * np.sqrt(np.sum((r2 - mean2) ** 2))\n",
                "\n",
                "    if denominator == 0:\n",
                "        return 0\n",
                "    return numerator / denominator"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-computation",
            "metadata": {},
            "source": [
                "## 3. Computing Similarity Matrices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "msd-computation",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- MSD Similarity ---\n",
                "msd_similarity = pd.DataFrame(index=rating_matrix.index, columns=rating_matrix.index)\n",
                "\n",
                "for u1 in rating_matrix.index:\n",
                "    for u2 in rating_matrix.index:\n",
                "        msd_similarity.loc[u1, u2] = mean_square_difference(\n",
                "            rating_matrix.loc[u1], rating_matrix.loc[u2]\n",
                "        )\n",
                "\n",
                "# Transform: lower MSD → higher similarity\n",
                "msd_similarity = 1 / (1 + msd_similarity.astype(float))\n",
                "msd_similarity.fillna(0, inplace=True)\n",
                "\n",
                "print(\"MSD similarity matrix computed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "pearson-computation",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Pearson Similarity ---\n",
                "pearson_similarity_matrix = pd.DataFrame(index=rating_matrix.index, columns=rating_matrix.index)\n",
                "\n",
                "for u1 in rating_matrix.index:\n",
                "    for u2 in rating_matrix.index:\n",
                "        if u1 != u2:\n",
                "            pearson_similarity_matrix.loc[u1, u2] = pearson_similarity(\n",
                "                rating_matrix.loc[u1], rating_matrix.loc[u2]\n",
                "            )\n",
                "        else:\n",
                "            pearson_similarity_matrix.loc[u1, u2] = 1  # Self-similarity\n",
                "\n",
                "pearson_similarity_matrix.fillna(0, inplace=True)\n",
                "\n",
                "# Normalize from [-1, 1] → [0, 1]\n",
                "pearson_similarity_matrix = (pearson_similarity_matrix.astype(float) + 1) / 2\n",
                "\n",
                "print(\"Pearson similarity matrix computed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cosine-computation",
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Cosine Similarity ---\n",
                "filled_rating_matrix = rating_matrix.fillna(0)\n",
                "\n",
                "cosine_similarity_matrix = pd.DataFrame(\n",
                "    cosine_similarity(filled_rating_matrix),\n",
                "    index=rating_matrix.index,\n",
                "    columns=rating_matrix.index\n",
                ")\n",
                "\n",
                "print(\"Cosine similarity matrix computed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-visualization",
            "metadata": {},
            "source": [
                "## 4. Visualization\n",
                "\n",
                "Heatmaps provide a quick visual inspection of how similar users are across the three metrics.  \n",
                "**Green** = high similarity, **Red** = low similarity."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "heatmap-config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Shared colormap: red (low) → white (mid) → green (high)\n",
                "cmap = LinearSegmentedColormap.from_list(\n",
                "    'green_red_white', ['red', 'white', 'green'], N=256\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "msd-heatmap",
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(\n",
                "    msd_similarity, annot=True, fmt='.2f', cmap=cmap,\n",
                "    center=0.5, linewidths=0.5,\n",
                "    cbar_kws={'label': 'MSD Similarity'}\n",
                ")\n",
                "plt.title(\"User–User MSD Similarity\")\n",
                "plt.xlabel(\"User ID\")\n",
                "plt.ylabel(\"User ID\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "pearson-heatmap",
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(\n",
                "    pearson_similarity_matrix, annot=True, fmt='.2f', cmap=cmap,\n",
                "    center=0.5, linewidths=0.5,\n",
                "    cbar_kws={'label': 'Pearson Similarity (normalized)'}\n",
                ")\n",
                "plt.title(\"User–User Pearson Similarity (Normalized to [0, 1])\")\n",
                "plt.xlabel(\"User ID\")\n",
                "plt.ylabel(\"User ID\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cosine-heatmap",
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(\n",
                "    cosine_similarity_matrix, annot=True, fmt='.2f', cmap=cmap,\n",
                "    center=0.5, linewidths=0.5,\n",
                "    cbar_kws={'label': 'Cosine Similarity'}\n",
                ")\n",
                "plt.title(\"User–User Cosine Similarity\")\n",
                "plt.xlabel(\"User ID\")\n",
                "plt.ylabel(\"User ID\")\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "distribution-plot",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Similarity score distributions (excluding self-similarity diagonal)\n",
                "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
                "\n",
                "for ax, (name, matrix) in zip(axes, [\n",
                "    ('MSD', msd_similarity),\n",
                "    ('Pearson', pearson_similarity_matrix),\n",
                "    ('Cosine', cosine_similarity_matrix)\n",
                "]):\n",
                "    vals = matrix.values[np.triu_indices_from(matrix.values, k=1)]\n",
                "    sns.histplot(vals, kde=True, ax=ax, color='steelblue', bins=15)\n",
                "    ax.set_title(f'{name} Distribution')\n",
                "    ax.set_xlabel('Similarity Score')\n",
                "    ax.set_ylabel('Frequency')\n",
                "\n",
                "plt.suptitle('Similarity Score Distributions (Upper Triangle)', y=1.02, fontsize=13)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-evaluation",
            "metadata": {},
            "source": [
                "## 5. Evaluation — k-NN Rating Prediction\n",
                "\n",
                "We evaluate each similarity metric by using a **k-Nearest Neighbors** approach to predict ratings:  \n",
                "for each known rating, we mask it, predict it using the top-*k* most similar users, and measure **MAE** and **RMSE**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "knn-prediction",
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_rating_knn(user_id, item_id, sim_matrix, rating_mat, k=5):\n",
                "    \"\"\"Predict a single rating using weighted k-NN.\"\"\"\n",
                "    similarities = sim_matrix.loc[user_id].drop(user_id)\n",
                "    item_ratings = rating_mat[item_id].drop(user_id)\n",
                "\n",
                "    # Keep only users who rated this item\n",
                "    rated_mask = item_ratings.notna()\n",
                "    if rated_mask.sum() == 0:\n",
                "        return np.nan\n",
                "\n",
                "    sim_scores = similarities[rated_mask].astype(float)\n",
                "    ratings = item_ratings[rated_mask].astype(float)\n",
                "\n",
                "    # Select top-k neighbors\n",
                "    top_k = sim_scores.nlargest(k)\n",
                "    top_ratings = ratings[top_k.index]\n",
                "\n",
                "    weight_sum = top_k.sum()\n",
                "    if weight_sum == 0:\n",
                "        return np.nan\n",
                "\n",
                "    return (top_k * top_ratings).sum() / weight_sum\n",
                "\n",
                "\n",
                "def evaluate_similarity(sim_matrix, rating_mat, k=5):\n",
                "    \"\"\"Evaluate a similarity matrix with leave-one-out on observed ratings.\"\"\"\n",
                "    actuals, predictions = [], []\n",
                "\n",
                "    for user_id in rating_mat.index:\n",
                "        rated_items = rating_mat.loc[user_id].dropna().index\n",
                "        for item_id in rated_items:\n",
                "            pred = predict_rating_knn(user_id, item_id, sim_matrix, rating_mat, k)\n",
                "            if not np.isnan(pred):\n",
                "                actuals.append(rating_mat.loc[user_id, item_id])\n",
                "                predictions.append(pred)\n",
                "\n",
                "    mae = mean_absolute_error(actuals, predictions)\n",
                "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
                "    return mae, rmse, len(actuals)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "evaluation-results",
            "metadata": {},
            "outputs": [],
            "source": [
                "results = {}\n",
                "for name, sim_mat in [('MSD', msd_similarity),\n",
                "                       ('Pearson', pearson_similarity_matrix),\n",
                "                       ('Cosine', cosine_similarity_matrix)]:\n",
                "    mae, rmse, n = evaluate_similarity(sim_mat, rating_matrix, k=5)\n",
                "    results[name] = {'MAE': mae, 'RMSE': rmse, 'Predictions': n}\n",
                "    print(f\"{name:>8s}  |  MAE: {mae:.4f}  |  RMSE: {rmse:.4f}  |  n={n}\")\n",
                "\n",
                "results_df = pd.DataFrame(results).T\n",
                "results_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "evaluation-barplot",
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
                "\n",
                "results_df['MAE'].plot.bar(ax=axes[0], color=['#e74c3c', '#3498db', '#2ecc71'], edgecolor='black')\n",
                "axes[0].set_title('MAE by Similarity Metric')\n",
                "axes[0].set_ylabel('Mean Absolute Error')\n",
                "axes[0].set_ylim(0, results_df['MAE'].max() * 1.3)\n",
                "axes[0].tick_params(axis='x', rotation=0)\n",
                "\n",
                "results_df['RMSE'].plot.bar(ax=axes[1], color=['#e74c3c', '#3498db', '#2ecc71'], edgecolor='black')\n",
                "axes[1].set_title('RMSE by Similarity Metric')\n",
                "axes[1].set_ylabel('Root Mean Squared Error')\n",
                "axes[1].set_ylim(0, results_df['RMSE'].max() * 1.3)\n",
                "axes[1].tick_params(axis='x', rotation=0)\n",
                "\n",
                "plt.suptitle('k-NN Rating Prediction Accuracy (k=5)', fontsize=13, y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "section-conclusion",
            "metadata": {},
            "source": [
                "## 6. Observations\n",
                "\n",
                "- **MSD** tends to produce the most uniform similarity scores — it treats all rating differences equally regardless of direction.\n",
                "- **Pearson** captures relative preference patterns (e.g., both users rate action movies higher than comedy) even when their absolute scales differ.\n",
                "- **Cosine** is sensitive to the magnitude of the rating vector, which can skew results for users with very few ratings.\n",
                "- The **k-NN evaluation** provides a quantitative comparison of how well each metric translates into actual rating predictions.\n",
                "\n",
                "For a deeper approach using generative models, see the companion **CFGAN notebook**."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
